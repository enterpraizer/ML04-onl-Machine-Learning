{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3cdfa380",
   "metadata": {},
   "source": [
    "\n",
    "# Конспект: Теория графов, алгоритмы и использование в ML/DL\n",
    "\n",
    "**Содержимое:**\n",
    "\n",
    "1. Краткое введение в теорию графов — основные понятия.\n",
    "2. Основные алгоритмы на графах и их практическое применение (краткие выжимки).\n",
    "3. Использование графов в ML/DL: графовые модели и сети (GCN, GAT, GraphSAGE и пр.).\n",
    "4. Практика: реализация класса `DecisionTree` (CART) для дальнейшего использования в RandomForest и Gradient Boosting.\n",
    "\n",
    "\n",
    "\n",
    "> Формат: Jupyter Notebook с текстом, схемами/кодом и примерами использования.\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "fdc625a03ad2139f"
  },
  {
   "cell_type": "markdown",
   "id": "790370a6",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Основные понятия теории графов\n",
    "\n",
    "- **Граф** — набор вершин (nodes) и ребер (edges).\n",
    "- **Ориентированный/неориентированный граф** — ребра имеют направление или нет.\n",
    "- **Взвешенный граф** — ребра имеют веса (cost, distance, capacity).\n",
    "- **Степень вершины (degree)** — число инцидентных ребер (для ориентированного — in-degree и out-degree).\n",
    "- **Путь, простй путь, цикл, простой цикл**.\n",
    "- **Компонента связности** — связная часть графа.\n",
    "- **Дерево** — связный ациклический граф. Для дерева на N вершинах всегда N-1 ребер.\n",
    "- **Лес** — набор деревьев.\n",
    "- **Корень дерева, листья, уровень, глубина**.\n",
    "- **Матрицы представления:** матрица смежности, матрица инцидентности, список смежности.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bde9dcf",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Основные алгоритмы на графах (выжимка)\n",
    "\n",
    "Ниже — краткое описание алгоритмов и их практического применения.\n",
    "\n",
    "### Поиск и обход\n",
    "- **BFS (Breadth-First Search)**: поиск в ширину, находит кратчайшие пути в невзвешенных графах (применение: поиск по уровням, расстояние в соц. сетях, маршрутизация, проверка связности).\n",
    "- **DFS (Depth-First Search)**: поиск в глубину, используется для топологической сортировки, обнаружения циклов, поиска компонент связности в ориентированных графах (с применением транспонированного графа — алгоритм Косараджу/Тарьяна для SCC).\n",
    "\n",
    "### Кратчайшие пути\n",
    "- **Dijkstra**: для неотрицательных весов, эффективно с кучей (priority queue). Применение: маршрутизация (GPS), планирование маршрутов, сетевая маршрутизация.\n",
    "- **Bellman-Ford**: корректен при отрицательных ребрах, обнаруживает отрицательные циклы. Применение: модели экономических зависимостей, некоторые разновидности оптимизаций.\n",
    "- **A\\***: эвристический поиск, использует функцию оценки f = g + h; широко применяется в pathfinding (игры, робототехника).\n",
    "\n",
    "### Остовные деревья (Minimum Spanning Tree)\n",
    "- **Kruskal**: сортировка ребер + DSU (union-find). Применение: проектирование сетей (минимальная длина кабелей), кластеризация (agglomerative).\n",
    "- **Prim**: растущий алгоритм, часто используется для плотных графов.\n",
    "\n",
    "### Потоки в сетях\n",
    "- **Edmonds-Karp / Ford-Fulkerson / Dinic**: максимальный поток, минимальный разрез. Применение: задачи распределения ресурсов, маршрутизация с ограничениями, вычисление пропускной способности.\n",
    "\n",
    "### Другие важные\n",
    "- **Топологическая сортировка** — для ориентированных ациклических графов (DAG); применение: планирование задач, разрешение зависимостей, сборка проектов.\n",
    "- **Алгоритмы для SCC (Tarjan, Kosaraju)** — обнаружение сильно связных компонент.\n",
    "- **PageRank** — ранжирование узлов (по ссылочной структуре), широко применяется в поисковых системах и анализе важности узлов в сетях.\n",
    "- **Алгоритмы на графах для динамических/потоковых данных** — approximate / streaming algorithms (например, для больших социальных графов).\n",
    "\n",
    "### Практическое применение (кейсы)\n",
    "- Социальные сети: рекомендации, поиск сообществ, влияние (centrality) — алгоритмы обхода, PageRank, community detection (Louvain, Girvan-Newman).\n",
    "- Транспорт и логистика: кратчайшие пути, оптимальные маршруты, пропускная способность дорог/сетей.\n",
    "- Биология: анализ взаимодействий белков (PPI), метаболические сети — поиск кластеров, классификация узлов.\n",
    "- Сети и телеком — маршрутизация, оптимизация трафика, отказоустойчивость.\n",
    "- Компиляция/билд-системы — DAG, топологическая сортировка.\n",
    "- Рекомендательные системы — графовые фичи и модели переходов.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98793d6",
   "metadata": {},
   "source": [
    "\n",
    "## 3. Графы в ML/DL — основные идеи и модели\n",
    "\n",
    "### Почему графы в ML важны?\n",
    "- Данные часто имеют структуру связей: соц. сети, молекулы (атома — вершины, связи — ребра), знания (knowledge graphs), дорожные сети.\n",
    "- Графовые подходы учитывают локальную структуру и зависимости между объектами.\n",
    "\n",
    "### Основные задачи\n",
    "- **Node classification** — классификация вершин (например, предсказание роли/типа в графе).\n",
    "- **Link prediction** — предсказание появления ребра между вершинами (рекомендации, предсказание взаимодействий).\n",
    "- **Graph classification** — классификация целого графа (например, токсичность молекулы).\n",
    "- **Graph regression** — регрессия по графам (энергия молекулы).\n",
    "\n",
    "### Популярные архитектуры (конспект)\n",
    "- **GCN (Graph Convolutional Network)** — сверточный слой на графе (Kipf & Welling). Успешно применялись для node classification и semi-supervised learning.\n",
    "- **GraphSAGE** — агрегация соседей (sample and aggregate) для работы с большими графами.\n",
    "- **GAT (Graph Attention Network)** — attention-механизм между соседями, улучшает агрегацию взвешенных вкладов соседей.\n",
    "- **Message Passing Neural Networks (MPNN)** — обобщённая схема, в которой узлы обмениваются сообщениями и обновляют состояния.\n",
    "- **Graph Transformers / Graphormer** — transformer-подобные архитектуры, адаптированные под графы (актуальная тема с сильными результатами в некоторых задачах).\n",
    "- **Graph Autoencoders / Variational GAE** — для встраивания вершин и задач link prediction.\n",
    "\n",
    "### Успешность применений (кратко)\n",
    "- GNNs дают значительный прирост на задачах, где структура графа критична: биоинформатика (включая предсказание взаимодействий и свойств молекул), социальная аналитика, системы рекомендаций (в комбинации с другими методами).\n",
    "- Ограничения: трудности с выражением дальних зависимостей в больших графах (over-smoothing), масштабируемость и чувствительность к качеству графа и меток.\n",
    "- Для реальных промышленных систем часто комбинируют графовые встраивания с классическими ML-фичами и используют методы масштабирования (GraphSAGE, sampling, mini-batching, Cluster-GCN).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83918370",
   "metadata": {},
   "source": [
    "\n",
    "## 4. Практическая часть — `DecisionTree` (CART) для классификации и регрессии\n",
    "\n",
    "Задача: реализовать класс дерева решений, который можно переиспользовать как базовую модель в RandomForest и Gradient Boosting.\n",
    "Требования к реализации (минимум):\n",
    "- Поддержка задач классификации (критерий Gini или Entropy) и регрессии (MSE).\n",
    "- Параметры: max_depth, min_samples_split, min_samples_leaf, max_features (опционально).\n",
    "- Простая реализация разбиений для числовых признаков (threshold по значениям).\n",
    "- Методы `fit(X, y)`, `predict(X)` и `predict_proba(X)` (для классификации).\n",
    "- Чистый Python, понятный код и несколько примеров использования.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d192ae3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Imports for examples and tests\n",
    "import numpy as np\n",
    "import math\n",
    "from collections import Counter, defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129431d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Пример: рисуем маленький взвешенный граф для иллюстрации\n",
    "G = nx.Graph()\n",
    "G.add_weighted_edges_from([('A','B',2), ('A','C',1), ('B','C',4), ('C','D',3), ('B','D',5)])\n",
    "pos = nx.spring_layout(G, seed=42)\n",
    "plt.figure(figsize=(5,4))\n",
    "nx.draw(G, pos, with_labels=True, node_size=600)\n",
    "labels = nx.get_edge_attributes(G,'weight')\n",
    "nx.draw_networkx_edge_labels(G, pos, edge_labels=labels)\n",
    "plt.title('Пример взвешенного графа')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a18dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DecisionTree:\n",
    "    \"\"\"\n",
    "    Простая реализация CART-дерева для задач классификации и регрессии.\n",
    "\n",
    "    Параметры:\n",
    "    - task: 'classification' или 'regression'\n",
    "    - max_depth, min_samples_split, min_samples_leaf, max_features (None или int/float)\n",
    "    - criterion: 'gini' / 'entropy' (for classification) or 'mse' for regression\n",
    "    \"\"\"\n",
    "    def __init__(self, task='classification', max_depth=6, min_samples_split=2, min_samples_leaf=1,\n",
    "                 max_features=None, criterion=None):\n",
    "        self.task = task\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.max_features = max_features\n",
    "        if criterion is None:\n",
    "            if task == 'classification':\n",
    "                self.criterion = 'gini'\n",
    "            else:\n",
    "                self.criterion = 'mse'\n",
    "        else:\n",
    "            self.criterion = criterion\n",
    "        self.root = None\n",
    "\n",
    "    class Node:\n",
    "        def __init__(self, *, feature=None, threshold=None, left=None, right=None, value=None, proba=None):\n",
    "            self.feature = feature\n",
    "            self.threshold = threshold\n",
    "            self.left = left\n",
    "            self.right = right\n",
    "            self.value = value  # prediction at leaf (class or regression value)\n",
    "            self.proba = proba  # for classification: class distribution at leaf\n",
    "\n",
    "    def _gini(self, y):\n",
    "        m = len(y)\n",
    "        if m == 0: return 0.0\n",
    "        counts = Counter(y)\n",
    "        return 1.0 - sum((c/m)**2 for c in counts.values())\n",
    "\n",
    "    def _entropy(self, y):\n",
    "        m = len(y)\n",
    "        if m == 0: return 0.0\n",
    "        counts = Counter(y)\n",
    "        ent = 0.0\n",
    "        for c in counts.values():\n",
    "            p = c/m\n",
    "            if p > 0:\n",
    "                ent -= p * math.log2(p)\n",
    "        return ent\n",
    "\n",
    "    def _mse(self, y):\n",
    "        if len(y) == 0: return 0.0\n",
    "        mu = sum(y)/len(y)\n",
    "        return sum((yy-mu)**2 for yy in y) / len(y)\n",
    "\n",
    "    def _impurity(self, y):\n",
    "        if self.task == 'classification':\n",
    "            if self.criterion == 'gini':\n",
    "                return self._gini(y)\n",
    "            else:\n",
    "                return self._entropy(y)\n",
    "        else:\n",
    "            return self._mse(y)\n",
    "\n",
    "    def _best_split(self, X, y):\n",
    "        # returns (feature, threshold, best_gain, left_idx, right_idx)\n",
    "        m, n = X.shape\n",
    "        if m < self.min_samples_split:\n",
    "            return None\n",
    "        parent_impurity = self._impurity(y)\n",
    "        best_gain = 0.0\n",
    "        best_feat = None\n",
    "        best_thresh = None\n",
    "        best_left_idx = None\n",
    "        best_right_idx = None\n",
    "\n",
    "        feat_indices = list(range(n))\n",
    "        if self.max_features is not None:\n",
    "            if isinstance(self.max_features, float):\n",
    "                k = max(1, int(self.max_features * n))\n",
    "            else:\n",
    "                k = int(self.max_features)\n",
    "            np.random.shuffle(feat_indices)\n",
    "            feat_indices = feat_indices[:k]\n",
    "\n",
    "        for feat in feat_indices:\n",
    "            vals = X[:, feat]\n",
    "            # consider unique sorted thresholds (midpoints)\n",
    "            uniq = np.unique(vals)\n",
    "            if len(uniq) == 1:\n",
    "                continue\n",
    "            thresholds = (uniq[:-1] + uniq[1:]) / 2.0\n",
    "            for thresh in thresholds:\n",
    "                left_mask = vals <= thresh\n",
    "                right_mask = ~left_mask\n",
    "                if left_mask.sum() < self.min_samples_leaf or right_mask.sum() < self.min_samples_leaf:\n",
    "                    continue\n",
    "                y_left = y[left_mask]\n",
    "                y_right = y[right_mask]\n",
    "                w_left = len(y_left) / m\n",
    "                w_right = 1 - w_left\n",
    "                impurity_left = self._impurity(y_left)\n",
    "                impurity_right = self._impurity(y_right)\n",
    "                gain = parent_impurity - (w_left * impurity_left + w_right * impurity_right)\n",
    "                if gain > best_gain:\n",
    "                    best_gain = gain\n",
    "                    best_feat = feat\n",
    "                    best_thresh = thresh\n",
    "                    best_left_idx = left_mask\n",
    "                    best_right_idx = right_mask\n",
    "\n",
    "        if best_feat is None:\n",
    "            return None\n",
    "        return best_feat, best_thresh, best_gain, best_left_idx, best_right_idx\n",
    "\n",
    "    def _make_leaf(self, y):\n",
    "        if self.task == 'classification':\n",
    "            counts = Counter(y)\n",
    "            total = len(y)\n",
    "            proba = {k: v/total for k,v in counts.items()}\n",
    "            # choose most common class as value\n",
    "            value = counts.most_common(1)[0][0]\n",
    "            return self.Node(value=value, proba=proba)\n",
    "        else:\n",
    "            value = float(sum(y)/len(y))\n",
    "            return self.Node(value=value, proba=None)\n",
    "\n",
    "    def _build(self, X, y, depth=0):\n",
    "        # stopping criteria\n",
    "        if depth >= self.max_depth or len(y) < self.min_samples_split or len(set(y)) == 1:\n",
    "            return self._make_leaf(y)\n",
    "        split = self._best_split(X, y)\n",
    "        if split is None:\n",
    "            return self._make_leaf(y)\n",
    "        feat, thresh, gain, left_mask, right_mask = split\n",
    "        left = self._build(X[left_mask], y[left_mask], depth+1)\n",
    "        right = self._build(X[right_mask], y[right_mask], depth+1)\n",
    "        return self.Node(feature=feat, threshold=thresh, left=left, right=right, value=None)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X = np.asarray(X)\n",
    "        y = np.asarray(y)\n",
    "        # For classification, convert string labels to ints if needed\n",
    "        if self.task == 'classification':\n",
    "            self._classes = list(np.unique(y))\n",
    "            # map to integers for internal handling\n",
    "            self._label_to_int = {c:i for i,c in enumerate(self._classes)}\n",
    "            self._int_to_label = {i:c for c,i in self._label_to_int.items()}\n",
    "            y_mapped = np.array([self._label_to_int[v] for v in y])\n",
    "            self.root = self._build(X, y_mapped, depth=0)\n",
    "        else:\n",
    "            self.root = self._build(X, y.astype(float), depth=0)\n",
    "        return self\n",
    "\n",
    "    def _predict_one(self, x, node):\n",
    "        if node.value is not None:\n",
    "            return node.value, node.proba\n",
    "        if x[node.feature] <= node.threshold:\n",
    "            return self._predict_one(x, node.left)\n",
    "        else:\n",
    "            return self._predict_one(x, node.right)\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = np.asarray(X)\n",
    "        preds = []\n",
    "        for x in X:\n",
    "            val, proba = self._predict_one(x, self.root)\n",
    "            if self.task == 'classification':\n",
    "                # val is int label\n",
    "                return_label = self._int_to_label[val]\n",
    "                preds.append(return_label)\n",
    "            else:\n",
    "                preds.append(val)\n",
    "        return np.array(preds)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        if self.task != 'classification':\n",
    "            raise ValueError('predict_proba is only for classification')\n",
    "        X = np.asarray(X)\n",
    "        out = []\n",
    "        for x in X:\n",
    "            val, proba = self._predict_one(x, self.root)\n",
    "            # convert proba dict with integer keys to original labels and vector\n",
    "            vec = {self._int_to_label[k]: v for k,v in (proba or {}).items()}\n",
    "            out.append(vec)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1f108f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Тест: классификация на Iris\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target  # already numeric labels 0,1,2\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "dt = DecisionTree(task='classification', max_depth=5, min_samples_split=2)\n",
    "dt.fit(X_train, y_train)\n",
    "y_pred = dt.predict(X_test)\n",
    "print('Iris accuracy (simple DecisionTree):', accuracy_score(y_test, y_pred))\n",
    "\n",
    "# Тест: регрессия на Diabetes\n",
    "diabetes = datasets.load_diabetes()\n",
    "X = diabetes.data\n",
    "y = diabetes.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "dt_reg = DecisionTree(task='regression', max_depth=6, min_samples_split=5)\n",
    "dt_reg.fit(X_train, y_train)\n",
    "y_pred_reg = dt_reg.predict(X_test)\n",
    "print('Diabetes MSE (simple DecisionTree):', mean_squared_error(y_test, y_pred_reg))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d591cf4",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### Что вы получили в этом ноутбуке\n",
    "- Краткий теоретический конспект по основам теории графов и популярным алгоритмам.\n",
    "- Краткая сводка по графовым методам в ML/DL и их практическим применениям.\n",
    "- Практическая реализация простого CART-дерева решений для классификации и регрессии, тесты на sklearn-данных.\n",
    "\n",
    "---\n",
    "\n",
    "Если хочешь, я могу:\n",
    "- Добавить объяснительные диаграммы (развёрнутые схемы алгоритмов: Dijkstra, Kruskal, GCN flow) как встраиваемые изображения.\n",
    "- Улучшить DecisionTree (поддержка категориальных признаков, ветвления по информационному приросту с энтропией, оптимизации скорости, компрессии узлов).\n",
    "- Реализовать RandomForest поверх этого дерева и простой GB (градиентный бустинг) на его основе.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
